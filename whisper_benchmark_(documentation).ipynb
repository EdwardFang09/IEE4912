{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyM6XsSSS/ZtUFdz1BCRsexF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EdwardFang09/IEE4912/blob/main/whisper_benchmark_(documentation).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> Benchmark (no early stopping)"
      ],
      "metadata": {
        "id": "eU99gaSN-8Ho"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suara direkam di lab ramai untuk simulasi."
      ],
      "metadata": {
        "id": "mVpxtyyBxPNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Library for quick-start on google colab\n",
        "!pip install faster-whisper jiwer nvidia-ml-py3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHcivJ9TB3zo",
        "outputId": "bc18fe2d-fa02-4ebe-defc-2034f2214ae1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: faster-whisper in /usr/local/lib/python3.11/dist-packages (1.1.1)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: nvidia-ml-py3 in /usr/local/lib/python3.11/dist-packages (7.352.0)\n",
            "Requirement already satisfied: ctranslate2<5,>=4.0 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (4.5.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.29.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (0.21.1)\n",
            "Requirement already satisfied: onnxruntime<2,>=1.14 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (1.21.0)\n",
            "Requirement already satisfied: av>=11 in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (14.2.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from faster-whisper) (4.67.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.12.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (75.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2024.12.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (24.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper) (4.12.2)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (5.29.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.13.1)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper) (10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2025.1.31)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "o6IJ0omtnnve",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4c216f6-a5bb-47a8-e965-bb92da1265cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: tiny, Compute: float32, Batch: 8, Time: 1.29s, WER: 3.00, Text:,  So, now we are going to connect the metter., Used memory (GB): 1.1522216796875\n",
            "Model: tiny, Compute: float32, Batch: 16, Time: 1.41s, WER: 3.00, Text:,  So, now we are going to connect the metter., Used memory (GB): 0.6092529296875\n",
            "Model: tiny, Compute: float32, Batch: 32, Time: 1.34s, WER: 3.00, Text:,  So, now we are going to connect the metter., Used memory (GB): 0.7030029296875\n",
            "Model: tiny, Compute: float16, Batch: 8, Time: 1.64s, WER: 3.00, Text:,  So, now we are going to connect the metter., Used memory (GB): 0.6405029296875\n",
            "Model: tiny, Compute: float16, Batch: 16, Time: 0.95s, WER: 3.00, Text:,  So, now we are going to connect the metter., Used memory (GB): 0.6092529296875\n",
            "Model: tiny, Compute: float16, Batch: 32, Time: 1.23s, WER: 3.00, Text:,  So, now we are going to connect the metter., Used memory (GB): 0.5780029296875\n",
            "Model: tiny, Compute: int8, Batch: 8, Time: 3.74s, WER: 3.00, Text:,  So, now we are going to connect the matter., Used memory (GB): 0.5780029296875\n",
            "Model: tiny, Compute: int8, Batch: 16, Time: 5.53s, WER: 3.00, Text:,  So, now we are going to connect the matter., Used memory (GB): 0.5780029296875\n",
            "Model: tiny, Compute: int8, Batch: 32, Time: 3.94s, WER: 3.00, Text:,  So, now we are going to connect the matter., Used memory (GB): 0.5780029296875\n",
            "Model: tiny, Compute: int8_float16, Batch: 8, Time: 1.18s, WER: 2.33, Text:,  So, now we're going to connect it., Used memory (GB): 0.4842529296875\n",
            "Model: tiny, Compute: int8_float16, Batch: 16, Time: 1.43s, WER: 2.33, Text:,  So, now we're going to connect it., Used memory (GB): 0.4842529296875\n",
            "Model: tiny, Compute: int8_float16, Batch: 32, Time: 1.98s, WER: 2.33, Text:,  So, now we're going to connect it., Used memory (GB): 0.4842529296875\n",
            "Model: base, Compute: float32, Batch: 4, Time: 1.34s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.7342529296875\n",
            "Model: base, Compute: float32, Batch: 4, Time: 2.47s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.7655029296875\n",
            "Model: base, Compute: float32, Batch: 4, Time: 1.30s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.7342529296875\n",
            "Model: base, Compute: float16, Batch: 4, Time: 0.79s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.6717529296875\n",
            "Model: base, Compute: float16, Batch: 4, Time: 0.79s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.6405029296875\n",
            "Model: base, Compute: float16, Batch: 4, Time: 0.80s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.6405029296875\n",
            "Model: base, Compute: int8_float16, Batch: 4, Time: 1.01s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.5155029296875\n",
            "Model: base, Compute: int8_float16, Batch: 4, Time: 1.05s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.5155029296875\n",
            "Model: base, Compute: int8_float16, Batch: 4, Time: 1.04s, WER: 3.33, Text:,  Sorry, I can't make it.  Sorry, I can't make it., Used memory (GB): 0.5155029296875\n",
            "Model: small, Compute: float32, Batch: 4, Time: 2.25s, WER: 0.00, Text:,  SORA OPEN MENTIMETER, Used memory (GB): 1.3592529296875\n",
            "Model: small, Compute: float32, Batch: 4, Time: 2.06s, WER: 0.00, Text:,  SORA OPEN MENTIMETER, Used memory (GB): 1.4530029296875\n",
            "Model: small, Compute: float32, Batch: 4, Time: 2.56s, WER: 0.00, Text:,  SORA OPEN MENTIMETER, Used memory (GB): 1.3592529296875\n",
            "Model: small, Compute: float16, Batch: 4, Time: 1.03s, WER: 0.00, Text:,  SORA OPEN MENTIMETER, Used memory (GB): 1.0155029296875\n",
            "Model: small, Compute: float16, Batch: 4, Time: 1.03s, WER: 0.00, Text:,  SORA OPEN MENTIMETER, Used memory (GB): 1.0467529296875\n",
            "Model: small, Compute: float16, Batch: 4, Time: 1.70s, WER: 0.00, Text:,  SORA OPEN MENTIMETER, Used memory (GB): 1.0155029296875\n",
            "Model: small, Compute: int8_float16, Batch: 4, Time: 2.27s, WER: 0.67, Text:,  SORA OPEN 20 METER, Used memory (GB): 0.6717529296875\n",
            "Model: small, Compute: int8_float16, Batch: 4, Time: 1.73s, WER: 0.67, Text:,  SORA OPEN 20 METER, Used memory (GB): 0.6717529296875\n",
            "Model: small, Compute: int8_float16, Batch: 4, Time: 1.81s, WER: 0.67, Text:,  SORA OPEN 20 METER, Used memory (GB): 0.7342529296875\n",
            "Model: medium, Compute: float32, Batch: 4, Time: 12.26s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 3.3905029296875\n",
            "Model: medium, Compute: float32, Batch: 4, Time: 7.08s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 3.3280029296875\n",
            "Model: medium, Compute: float32, Batch: 4, Time: 4.61s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 3.6092529296875\n",
            "Model: medium, Compute: float16, Batch: 4, Time: 1.89s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 2.0155029296875\n",
            "Model: medium, Compute: float16, Batch: 4, Time: 1.76s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 2.0155029296875\n",
            "Model: medium, Compute: float16, Batch: 4, Time: 2.19s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 2.0155029296875\n",
            "Model: medium, Compute: int8_float16, Batch: 4, Time: 4.17s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 1.1717529296875\n",
            "Model: medium, Compute: int8_float16, Batch: 4, Time: 5.41s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 1.1717529296875\n",
            "Model: medium, Compute: int8_float16, Batch: 4, Time: 3.83s, WER: 0.67, Text:,  Sora open 90 meter, Used memory (GB): 1.1717529296875\n",
            "Model: large-v2, Compute: float32, Batch: 4, Time: 27.20s, WER: 0.00, Text:,  SORA, OPEN MENTIMETER, Used memory (GB): 6.7030029296875\n",
            "Model: large-v2, Compute: float32, Batch: 4, Time: 26.89s, WER: 1.00, Text:,  you, Used memory (GB): 6.9530029296875\n",
            "Model: large-v2, Compute: float32, Batch: 4, Time: 26.25s, WER: 1.00, Text:,  you, Used memory (GB): 6.9842529296875\n",
            "Model: large-v2, Compute: float16, Batch: 4, Time: 12.49s, WER: 4.67, Text:,  a  a  a  a  a  a  a  a  a  a  a  a  a  a, Used memory (GB): 3.4842529296875\n",
            "Model: large-v2, Compute: float16, Batch: 4, Time: 6.13s, WER: 1.00, Text:,  you, Used memory (GB): 3.4842529296875\n",
            "Model: large-v2, Compute: float16, Batch: 4, Time: 8.64s, WER: 1.00, Text:,  you, Used memory (GB): 3.5780029296875\n",
            "Model: large-v2, Compute: int8_float16, Batch: 4, Time: 9.78s, WER: 5.67, Text:,  So, that's it for this video guys, thanks for watching and I'll see you guys next time., Used memory (GB): 2.2030029296875\n",
            "Model: large-v2, Compute: int8_float16, Batch: 4, Time: 8.06s, WER: 5.67, Text:,  So, that's it for this video guys, thanks for watching and I'll see you guys next time., Used memory (GB): 2.1092529296875\n",
            "Model: large-v2, Compute: int8_float16, Batch: 4, Time: 8.70s, WER: 5.67, Text:,  So, that's it for this video guys, thanks for watching and I'll see you guys next time., Used memory (GB): 2.1092529296875\n",
            "Model: large-v3, Compute: float32, Batch: 4, Time: 23.67s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 6.5155029296875\n",
            "Model: large-v3, Compute: float32, Batch: 4, Time: 24.42s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 6.2655029296875\n",
            "Model: large-v3, Compute: float32, Batch: 4, Time: 26.70s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 6.2655029296875\n",
            "Model: large-v3, Compute: float16, Batch: 4, Time: 10.37s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 3.4842529296875\n",
            "Model: large-v3, Compute: float16, Batch: 4, Time: 3.43s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 3.7030029296875\n",
            "Model: large-v3, Compute: float16, Batch: 4, Time: 3.92s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 3.4842529296875\n",
            "Model: large-v3, Compute: int8_float16, Batch: 4, Time: 7.87s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 1.9217529296875\n",
            "Model: large-v3, Compute: int8_float16, Batch: 4, Time: 6.84s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 1.9217529296875\n",
            "Model: large-v3, Compute: int8_float16, Batch: 4, Time: 7.62s, WER: 1.00, Text:,  Sorry, open 20 meter., Used memory (GB): 1.9217529296875\n",
            "Model: turbo, Compute: float16, Batch: 8, Time: 10.22s, WER: 1.67, Text:,  I'm sorry about 20 meters, Used memory (GB): 2.1092529296875\n",
            "Model: turbo, Compute: float16, Batch: 8, Time: 3.50s, WER: 1.67, Text:,  I'm sorry about 20 meters, Used memory (GB): 2.1092529296875\n",
            "Model: turbo, Compute: float16, Batch: 8, Time: 2.35s, WER: 1.67, Text:,  I'm sorry about 20 meters, Used memory (GB): 2.1092529296875\n",
            "   model_size  compute_type  batch_size  inference_time language  \\\n",
            "0        tiny       float32           8        1.291853       en   \n",
            "1        tiny       float32          16        1.405953       en   \n",
            "2        tiny       float32          32        1.339309       en   \n",
            "3        tiny       float16           8        1.644244       en   \n",
            "4        tiny       float16          16        0.952217       en   \n",
            "5        tiny       float16          32        1.233922       en   \n",
            "6        tiny          int8           8        3.735388       en   \n",
            "7        tiny          int8          16        5.534463       en   \n",
            "8        tiny          int8          32        3.944243       en   \n",
            "9        tiny  int8_float16           8        1.177412       en   \n",
            "10       tiny  int8_float16          16        1.428185       en   \n",
            "11       tiny  int8_float16          32        1.982934       en   \n",
            "12       base       float32           4        1.342214       en   \n",
            "13       base       float32           4        2.473325       en   \n",
            "14       base       float32           4        1.302596       en   \n",
            "15       base       float16           4        0.791940       en   \n",
            "16       base       float16           4        0.792233       en   \n",
            "17       base       float16           4        0.798508       en   \n",
            "18       base  int8_float16           4        1.010075       en   \n",
            "19       base  int8_float16           4        1.054991       en   \n",
            "20       base  int8_float16           4        1.035873       en   \n",
            "21      small       float32           4        2.248011       en   \n",
            "22      small       float32           4        2.064276       en   \n",
            "23      small       float32           4        2.561359       en   \n",
            "24      small       float16           4        1.028663       en   \n",
            "25      small       float16           4        1.025427       en   \n",
            "26      small       float16           4        1.699510       en   \n",
            "27      small  int8_float16           4        2.266962       en   \n",
            "28      small  int8_float16           4        1.728993       en   \n",
            "29      small  int8_float16           4        1.808716       en   \n",
            "30     medium       float32           4       12.259716       en   \n",
            "31     medium       float32           4        7.079978       en   \n",
            "32     medium       float32           4        4.608215       en   \n",
            "33     medium       float16           4        1.893227       en   \n",
            "34     medium       float16           4        1.764809       en   \n",
            "35     medium       float16           4        2.185951       en   \n",
            "36     medium  int8_float16           4        4.165197       en   \n",
            "37     medium  int8_float16           4        5.411271       en   \n",
            "38     medium  int8_float16           4        3.830456       en   \n",
            "39   large-v2       float32           4       27.199498       en   \n",
            "40   large-v2       float32           4       26.891430       en   \n",
            "41   large-v2       float32           4       26.245283       en   \n",
            "42   large-v2       float16           4       12.490271       en   \n",
            "43   large-v2       float16           4        6.126460       en   \n",
            "44   large-v2       float16           4        8.641015       en   \n",
            "45   large-v2  int8_float16           4        9.779318       en   \n",
            "46   large-v2  int8_float16           4        8.061299       en   \n",
            "47   large-v2  int8_float16           4        8.702307       en   \n",
            "48   large-v3       float32           4       23.674452       en   \n",
            "49   large-v3       float32           4       24.424664       en   \n",
            "50   large-v3       float32           4       26.699106       en   \n",
            "51   large-v3       float16           4       10.372704       en   \n",
            "52   large-v3       float16           4        3.433218       en   \n",
            "53   large-v3       float16           4        3.924830       en   \n",
            "54   large-v3  int8_float16           4        7.871025       en   \n",
            "55   large-v3  int8_float16           4        6.844462       en   \n",
            "56   large-v3  int8_float16           4        7.621593       en   \n",
            "57      turbo       float16           8       10.218024       en   \n",
            "58      turbo       float16           8        3.504214       en   \n",
            "59      turbo       float16           8        2.346475       en   \n",
            "\n",
            "    language_probability  num_segments       wer  \\\n",
            "0                      1             1  3.000000   \n",
            "1                      1             1  3.000000   \n",
            "2                      1             1  3.000000   \n",
            "3                      1             1  3.000000   \n",
            "4                      1             1  3.000000   \n",
            "5                      1             1  3.000000   \n",
            "6                      1             1  3.000000   \n",
            "7                      1             1  3.000000   \n",
            "8                      1             1  3.000000   \n",
            "9                      1             1  2.333333   \n",
            "10                     1             1  2.333333   \n",
            "11                     1             1  2.333333   \n",
            "12                     1             2  3.333333   \n",
            "13                     1             2  3.333333   \n",
            "14                     1             2  3.333333   \n",
            "15                     1             2  3.333333   \n",
            "16                     1             2  3.333333   \n",
            "17                     1             2  3.333333   \n",
            "18                     1             2  3.333333   \n",
            "19                     1             2  3.333333   \n",
            "20                     1             2  3.333333   \n",
            "21                     1             1  0.000000   \n",
            "22                     1             1  0.000000   \n",
            "23                     1             1  0.000000   \n",
            "24                     1             1  0.000000   \n",
            "25                     1             1  0.000000   \n",
            "26                     1             1  0.000000   \n",
            "27                     1             1  0.666667   \n",
            "28                     1             1  0.666667   \n",
            "29                     1             1  0.666667   \n",
            "30                     1             1  0.666667   \n",
            "31                     1             1  0.666667   \n",
            "32                     1             1  0.666667   \n",
            "33                     1             1  0.666667   \n",
            "34                     1             1  0.666667   \n",
            "35                     1             1  0.666667   \n",
            "36                     1             1  0.666667   \n",
            "37                     1             1  0.666667   \n",
            "38                     1             1  0.666667   \n",
            "39                     1             1  0.000000   \n",
            "40                     1             1  1.000000   \n",
            "41                     1             1  1.000000   \n",
            "42                     1            14  4.666667   \n",
            "43                     1             1  1.000000   \n",
            "44                     1             1  1.000000   \n",
            "45                     1             1  5.666667   \n",
            "46                     1             1  5.666667   \n",
            "47                     1             1  5.666667   \n",
            "48                     1             1  1.000000   \n",
            "49                     1             1  1.000000   \n",
            "50                     1             1  1.000000   \n",
            "51                     1             1  1.000000   \n",
            "52                     1             1  1.000000   \n",
            "53                     1             1  1.000000   \n",
            "54                     1             1  1.000000   \n",
            "55                     1             1  1.000000   \n",
            "56                     1             1  1.000000   \n",
            "57                     1             1  1.666667   \n",
            "58                     1             1  1.666667   \n",
            "59                     1             1  1.666667   \n",
            "\n",
            "                                 predicted_transcript  Used memory (GB)  \n",
            "0         So, now we are going to connect the metter.          1.152222  \n",
            "1         So, now we are going to connect the metter.          0.609253  \n",
            "2         So, now we are going to connect the metter.          0.703003  \n",
            "3         So, now we are going to connect the metter.          0.640503  \n",
            "4         So, now we are going to connect the metter.          0.609253  \n",
            "5         So, now we are going to connect the metter.          0.578003  \n",
            "6         So, now we are going to connect the matter.          0.578003  \n",
            "7         So, now we are going to connect the matter.          0.578003  \n",
            "8         So, now we are going to connect the matter.          0.578003  \n",
            "9                  So, now we're going to connect it.          0.484253  \n",
            "10                 So, now we're going to connect it.          0.484253  \n",
            "11                 So, now we're going to connect it.          0.484253  \n",
            "12   Sorry, I can't make it.  Sorry, I can't make it.          0.734253  \n",
            "13   Sorry, I can't make it.  Sorry, I can't make it.          0.765503  \n",
            "14   Sorry, I can't make it.  Sorry, I can't make it.          0.734253  \n",
            "15   Sorry, I can't make it.  Sorry, I can't make it.          0.671753  \n",
            "16   Sorry, I can't make it.  Sorry, I can't make it.          0.640503  \n",
            "17   Sorry, I can't make it.  Sorry, I can't make it.          0.640503  \n",
            "18   Sorry, I can't make it.  Sorry, I can't make it.          0.515503  \n",
            "19   Sorry, I can't make it.  Sorry, I can't make it.          0.515503  \n",
            "20   Sorry, I can't make it.  Sorry, I can't make it.          0.515503  \n",
            "21                               SORA OPEN MENTIMETER          1.359253  \n",
            "22                               SORA OPEN MENTIMETER          1.453003  \n",
            "23                               SORA OPEN MENTIMETER          1.359253  \n",
            "24                               SORA OPEN MENTIMETER          1.015503  \n",
            "25                               SORA OPEN MENTIMETER          1.046753  \n",
            "26                               SORA OPEN MENTIMETER          1.015503  \n",
            "27                                 SORA OPEN 20 METER          0.671753  \n",
            "28                                 SORA OPEN 20 METER          0.671753  \n",
            "29                                 SORA OPEN 20 METER          0.734253  \n",
            "30                                 Sora open 90 meter          3.390503  \n",
            "31                                 Sora open 90 meter          3.328003  \n",
            "32                                 Sora open 90 meter          3.609253  \n",
            "33                                 Sora open 90 meter          2.015503  \n",
            "34                                 Sora open 90 meter          2.015503  \n",
            "35                                 Sora open 90 meter          2.015503  \n",
            "36                                 Sora open 90 meter          1.171753  \n",
            "37                                 Sora open 90 meter          1.171753  \n",
            "38                                 Sora open 90 meter          1.171753  \n",
            "39                              SORA, OPEN MENTIMETER          6.703003  \n",
            "40                                                you          6.953003  \n",
            "41                                                you          6.984253  \n",
            "42           a  a  a  a  a  a  a  a  a  a  a  a  a  a          3.484253  \n",
            "43                                                you          3.484253  \n",
            "44                                                you          3.578003  \n",
            "45   So, that's it for this video guys, thanks for...          2.203003  \n",
            "46   So, that's it for this video guys, thanks for...          2.109253  \n",
            "47   So, that's it for this video guys, thanks for...          2.109253  \n",
            "48                              Sorry, open 20 meter.          6.515503  \n",
            "49                              Sorry, open 20 meter.          6.265503  \n",
            "50                              Sorry, open 20 meter.          6.265503  \n",
            "51                              Sorry, open 20 meter.          3.484253  \n",
            "52                              Sorry, open 20 meter.          3.703003  \n",
            "53                              Sorry, open 20 meter.          3.484253  \n",
            "54                              Sorry, open 20 meter.          1.921753  \n",
            "55                              Sorry, open 20 meter.          1.921753  \n",
            "56                              Sorry, open 20 meter.          1.921753  \n",
            "57                          I'm sorry about 20 meters          2.109253  \n",
            "58                          I'm sorry about 20 meters          2.109253  \n",
            "59                          I'm sorry about 20 meters          2.109253  \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
        "import pandas as pd\n",
        "import jiwer\n",
        "import torch\n",
        "from jiwer import transforms\n",
        "import nvidia_smi\n",
        "\n",
        "# Define audio file and its ground truth transcript\n",
        "audio_file = \"edwardmentimeter.m4a\"  # Replace with your audio file\n",
        "ground_truth_transcript = \"sora, open mentimeter\"  # Replace with the actual transcript\n",
        "\n",
        "# Model sizes and compute types to benchmark\n",
        "model_configs = [\n",
        "    {\"size\": \"tiny\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"tiny\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"tiny\", \"compute\": \"int8\"},  # CPU INT8\n",
        "    {\"size\": \"tiny\", \"compute\": \"int8_float16\"}, # GPU INT8\n",
        "    {\"size\": \"base\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"base\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"base\", \"compute\": \"int8_float16\"}, # GPU INT8\n",
        "    {\"size\": \"small\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"small\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"small\", \"compute\": \"int8_float16\"}, # GPU INT8\n",
        "    {\"size\": \"medium\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"medium\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"medium\", \"compute\": \"int8_float16\"}, # GPU INT8\n",
        "    {\"size\": \"large-v2\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"large-v2\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"large-v2\", \"compute\": \"int8_float16\"}, # GPU INT8\n",
        "    {\"size\": \"large-v3\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"large-v3\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"large-v3\", \"compute\": \"int8_float16\"}, # GPU INT8\n",
        "    {\"size\": \"turbo\", \"compute\": \"float16\"},  # Turbo only supports FP16\n",
        "]\n",
        "\n",
        "batch_sizes =  [8, 16, 32]# Experiment with batch sizes\n",
        "\n",
        "results = []\n",
        "\n",
        "for config in model_configs:\n",
        "    for batch_size in batch_sizes:\n",
        "        if config[\"size\"] in [\"base\", \"small\", \"medium\", \"large-v2\", \"large-v3\"]:\n",
        "            effective_batch_size = min(batch_size, 4)  # Reduce for larger models\n",
        "        elif config[\"size\"] == \"turbo\":\n",
        "             effective_batch_size = min(batch_size, 8)\n",
        "        else:\n",
        "            effective_batch_size = batch_size\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            device = \"cuda\" if config[\"compute\"]!= \"int8\" else \"cpu\"\n",
        "            model = WhisperModel(config[\"size\"], device=device, compute_type=config[\"compute\"])\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                nvidia_smi.nvmlInit()\n",
        "                handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "                # No memory info printed here\n",
        "\n",
        "            if config[\"size\"] == \"turbo\":\n",
        "                batched_model = BatchedInferencePipeline(model=model)\n",
        "                segments, info = batched_model.transcribe(audio_file, batch_size=effective_batch_size, language='en')\n",
        "            else:\n",
        "                segments, info = model.transcribe(audio_file, beam_size=5, language='en')\n",
        "\n",
        "            segments_list = list(segments)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToLowerCase(),\n",
        "                transforms.RemovePunctuation(),\n",
        "                transforms.RemoveMultipleSpaces(),\n",
        "                transforms.Strip(),\n",
        "            ])\n",
        "\n",
        "            ground_truth_transformed = transform(ground_truth_transcript)\n",
        "            predicted_transcript = \" \".join([segment.text for segment in segments_list])\n",
        "            predicted_transcript_transformed = transform(predicted_transcript)\n",
        "\n",
        "\n",
        "            wer = jiwer.wer(ground_truth_transformed, predicted_transcript_transformed)\n",
        "\n",
        "            end_time = time.time()\n",
        "            inference_time = end_time - start_time\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                infoo = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "\n",
        "            results.append({\n",
        "                \"model_size\": config[\"size\"],\n",
        "                \"compute_type\": config[\"compute\"],\n",
        "                \"batch_size\": effective_batch_size,\n",
        "                \"inference_time\": inference_time,\n",
        "                \"language\": info.language,\n",
        "                \"language_probability\": info.language_probability,\n",
        "                \"num_segments\": len(segments_list),\n",
        "                \"wer\": wer,\n",
        "                \"predicted_transcript\": predicted_transcript, #added predicted transcript\n",
        "                \"Used memory (GB)\": infoo.used / (1024 ** 3),\n",
        "            })\n",
        "\n",
        "            print(f\"Model: {config['size']}, Compute: {config['compute']}, Batch: {effective_batch_size}, Time: {inference_time:.2f}s, WER: {wer:.2f}, Text:, {predicted_transcript}, Used memory (GB): {infoo.used / (1024 ** 3)}\") # added predicted transcript to the output\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error with Model: {config['size']}, Compute: {config['compute']}, Batch: {effective_batch_size}: {e}\")\n",
        "            results.append({\n",
        "                \"model_size\": config[\"size\"],\n",
        "                \"compute_type\": config[\"compute\"],\n",
        "                \"batch_size\": effective_batch_size,\n",
        "                \"inference_time\": \"Error\",\n",
        "                \"error\": str(e),\n",
        "                \"wer\": \"Error\",\n",
        "                \"predicted_transcript\": \"Error\" # added predicted transcript in case of error\n",
        "            })\n",
        "\n",
        "        finally:\n",
        "            del model\n",
        "            if 'batched_model' in locals():\n",
        "                del batched_model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Print or save results (e.g., to a CSV file)\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"no_stop_whisper_benchmark.csv\", index=False)\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulan: tidak perlu model besar. Model small bahkan sudah cukup untuk akurasi 100% dengan pemakaian memori ~1GB juga dengan waktu proses <2 detik"
      ],
      "metadata": {
        "id": "MJMHKnfXxbaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 >with early stopping (all models)"
      ],
      "metadata": {
        "id": "RrqyZja5Enz7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
        "import pandas as pd\n",
        "import jiwer\n",
        "import nvidia_smi\n",
        "from jiwer import transforms\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Define audio files and their ground truth transcripts (dictionary)\n",
        "audio_ground_truth = {\n",
        "    \"edwardchrome.m4a\": \"hey sora, open chrome.\",\n",
        "    \"edwardkahoot.m4a\": \"hey sora, open kahoot.\",\n",
        "    #... more audio files and transcripts\n",
        "}\n",
        "\n",
        "# Model sizes and compute types to benchmark\n",
        "model_configs = [\n",
        "    {\"size\": \"tiny\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"tiny\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"tiny\", \"compute\": \"int8\"},  # CPU INT8\n",
        "    {\"size\": \"base\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"base\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"small\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"small\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"medium\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"medium\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"turbo\", \"compute\": \"float16\"},  # Turbo only supports FP16\n",
        "    {\"size\": \"large-v2\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"large-v2\", \"compute\": \"float16\"},\n",
        "    {\"size\": \"large-v3\", \"compute\": \"float32\"},\n",
        "    {\"size\": \"large-v3\", \"compute\": \"float16\"},\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "i = 0\n",
        "for audio_file, ground_truth_transcript in audio_ground_truth.items():\n",
        "    for config in model_configs:\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            device = \"cuda\" if config[\"compute\"]!= \"int8\" else \"cpu\"\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                nvidia_smi.nvmlInit()\n",
        "                handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "                # No memory info printed here\n",
        "\n",
        "            model = WhisperModel(config[\"size\"], device=device, compute_type=config[\"compute\"])\n",
        "\n",
        "            if config[\"size\"] == \"turbo\":\n",
        "                batched_model = BatchedInferencePipeline(model=model)\n",
        "                segments, info = batched_model.transcribe(audio_file, language='en')\n",
        "            else:\n",
        "                segments, info = model.transcribe(audio_file, beam_size=5, language='en')\n",
        "\n",
        "            segments_list = list(segments)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToLowerCase(),\n",
        "                transforms.RemovePunctuation(),\n",
        "                transforms.RemoveMultipleSpaces(),\n",
        "                transforms.Strip(),\n",
        "            ])\n",
        "\n",
        "            ground_truth_transformed = transform(ground_truth_transcript)\n",
        "            predicted_transcript = \" \".join([segment.text for segment in segments_list])\n",
        "            predicted_transcript_transformed = transform(predicted_transcript)\n",
        "\n",
        "            wer = jiwer.wer(ground_truth_transformed, predicted_transcript_transformed)\n",
        "\n",
        "            end_time = time.time()\n",
        "            inference_time = end_time - start_time\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                infoo = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "\n",
        "            results.append({\n",
        "                \"audio_file\": audio_file,\n",
        "                \"model_size\": config[\"size\"],\n",
        "                \"compute_type\": config[\"compute\"],\n",
        "                \"inference_time\": inference_time,\n",
        "                \"language\": info.language,\n",
        "                \"language_probability\": info.language_probability,\n",
        "                \"num_segments\": len(segments_list),\n",
        "                \"wer\": wer,\n",
        "                \"predicted_transcript\": predicted_transcript,\n",
        "                \"Used memory (GB)\": infoo.used / (1024 ** 3)\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"Audio: {audio_file}, Model: {config['size']}, Compute: {config['compute']}, Time: {inference_time:.2f}s, WER: {wer:.2f}, Text:, {predicted_transcript}, Used memory (GB): {infoo.used / (1024 ** 3)}\"\n",
        "            )\n",
        "\n",
        "            if wer == 0:\n",
        "                if i == 0:\n",
        "                  i += 1 #add one more iteration for safety measure\n",
        "                else:\n",
        "                  print(f\"Skipping remaining models for this audio\")\n",
        "                  break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error with Audio: {audio_file}, Model: {config['size']}, Compute: {config['compute']}: {e}\"\n",
        "            )\n",
        "            results.append({\n",
        "                \"audio_file\": audio_file,\n",
        "                \"model_size\": config[\"size\"],\n",
        "                \"compute_type\": config[\"compute\"],\n",
        "                \"inference_time\": \"Error\",\n",
        "                \"error\": str(e),\n",
        "                \"wer\": \"Error\",\n",
        "                \"predicted_transcript\": \"Error\",\n",
        "            })\n",
        "\n",
        "        finally:\n",
        "            del model\n",
        "            if config[\"size\"] == \"turbo\":\n",
        "                del batched_model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Print or save results (e.g., to a CSV file)\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"whisper_benchmark_results.csv\", index=False)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "i0uMpwsB64YI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b4a8057-685f-4a35-acb1-9fcdcda1f886"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Audio: edwardchrome.m4a, Model: tiny, Compute: float32, Time: 1.57s, WER: 0.00, Text:,  Hey Sora, open Chrome., Used memory (GB): 0.6092529296875\n",
            "Audio: edwardchrome.m4a, Model: tiny, Compute: float16, Time: 0.90s, WER: 0.00, Text:,  Hey Sora, open Chrome., Used memory (GB): 0.5780029296875\n",
            "Skipping remaining models for this audio\n",
            "Audio: edwardkahoot.m4a, Model: tiny, Compute: float32, Time: 0.80s, WER: 0.50, Text:,  Hey Sora, open the hood., Used memory (GB): 0.6405029296875\n",
            "Audio: edwardkahoot.m4a, Model: tiny, Compute: float16, Time: 0.66s, WER: 0.50, Text:,  Hey Sora, open the hood., Used memory (GB): 0.6405029296875\n",
            "Audio: edwardkahoot.m4a, Model: tiny, Compute: int8, Time: 2.36s, WER: 0.50, Text:,  Hey Sora, open the hood., Used memory (GB): 0.6405029296875\n",
            "Audio: edwardkahoot.m4a, Model: base, Compute: float32, Time: 2.66s, WER: 0.25, Text:,  Hey Sora, open Kakut., Used memory (GB): 0.7342529296875\n",
            "Audio: edwardkahoot.m4a, Model: base, Compute: float16, Time: 0.76s, WER: 0.25, Text:,  Hey Sora, open Kakut., Used memory (GB): 0.6405029296875\n",
            "Audio: edwardkahoot.m4a, Model: small, Compute: float32, Time: 4.38s, WER: 0.25, Text:,  Hey Sora! Open Kakut!, Used memory (GB): 1.3592529296875\n",
            "Audio: edwardkahoot.m4a, Model: small, Compute: float16, Time: 1.08s, WER: 0.25, Text:,  Hey Sora! Open Kakut!, Used memory (GB): 1.0155029296875\n",
            "Audio: edwardkahoot.m4a, Model: medium, Compute: float32, Time: 12.21s, WER: 0.00, Text:,  Hey Sora, open Kahoot!, Used memory (GB): 3.3280029296875\n",
            "Skipping remaining models for this audio\n",
            "         audio_file model_size compute_type  inference_time language  \\\n",
            "0  edwardchrome.m4a       tiny      float32        1.570366       en   \n",
            "1  edwardchrome.m4a       tiny      float16        0.902620       en   \n",
            "2  edwardkahoot.m4a       tiny      float32        0.803736       en   \n",
            "3  edwardkahoot.m4a       tiny      float16        0.662070       en   \n",
            "4  edwardkahoot.m4a       tiny         int8        2.360618       en   \n",
            "5  edwardkahoot.m4a       base      float32        2.657722       en   \n",
            "6  edwardkahoot.m4a       base      float16        0.756444       en   \n",
            "7  edwardkahoot.m4a      small      float32        4.383519       en   \n",
            "8  edwardkahoot.m4a      small      float16        1.082096       en   \n",
            "9  edwardkahoot.m4a     medium      float32       12.205427       en   \n",
            "\n",
            "   language_probability  num_segments   wer       predicted_transcript  \\\n",
            "0                     1             1  0.00     Hey Sora, open Chrome.   \n",
            "1                     1             1  0.00     Hey Sora, open Chrome.   \n",
            "2                     1             1  0.50   Hey Sora, open the hood.   \n",
            "3                     1             1  0.50   Hey Sora, open the hood.   \n",
            "4                     1             1  0.50   Hey Sora, open the hood.   \n",
            "5                     1             1  0.25      Hey Sora, open Kakut.   \n",
            "6                     1             1  0.25      Hey Sora, open Kakut.   \n",
            "7                     1             1  0.25      Hey Sora! Open Kakut!   \n",
            "8                     1             1  0.25      Hey Sora! Open Kakut!   \n",
            "9                     1             1  0.00     Hey Sora, open Kahoot!   \n",
            "\n",
            "   Used memory (GB)  \n",
            "0          0.609253  \n",
            "1          0.578003  \n",
            "2          0.640503  \n",
            "3          0.640503  \n",
            "4          0.640503  \n",
            "5          0.734253  \n",
            "6          0.640503  \n",
            "7          1.359253  \n",
            "8          1.015503  \n",
            "9          3.328003  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1> turbo model (special model - optimized large)"
      ],
      "metadata": {
        "id": "60wG3KL0JpnI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
        "import pandas as pd\n",
        "import jiwer\n",
        "import nvidia_smi\n",
        "from jiwer import transforms\n",
        "import os\n",
        "import torch\n",
        "\n",
        "# Define audio files and their ground truth transcripts (dictionary)\n",
        "audio_ground_truth = {\n",
        "    \"edwardchrome.m4a\": \"hey sora, open chrome.\",\n",
        "    \"edwardkahoot.m4a\": \"hey sora, open kahoot.\",\n",
        "    #... more audio files and transcripts\n",
        "}\n",
        "\n",
        "# Model sizes and compute types to benchmark\n",
        "model_configs = [\n",
        "    {\"size\": \"turbo\", \"compute\": \"float16\"},  # Turbo only supports FP16\n",
        "]\n",
        "\n",
        "results = []\n",
        "\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "\n",
        "for audio_file, ground_truth_transcript in audio_ground_truth.items():\n",
        "    for config in model_configs:\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            device = \"cuda\" if config[\"compute\"]!= \"int8\" else \"cpu\"\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                nvidia_smi.nvmlInit()\n",
        "                handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "                # No memory info printed here\n",
        "\n",
        "            model = WhisperModel(config[\"size\"], device=device, compute_type=config[\"compute\"])\n",
        "\n",
        "            if config[\"size\"] == \"turbo\":\n",
        "                batched_model = BatchedInferencePipeline(model=model)\n",
        "                segments, info = batched_model.transcribe(audio_file, language='en')\n",
        "            else:\n",
        "                segments, info = model.transcribe(audio_file, beam_size=5, language='en')\n",
        "\n",
        "            segments_list = list(segments)\n",
        "\n",
        "            transform = transforms.Compose([\n",
        "                transforms.ToLowerCase(),\n",
        "                transforms.RemovePunctuation(),\n",
        "                transforms.RemoveMultipleSpaces(),\n",
        "                transforms.Strip(),\n",
        "            ])\n",
        "\n",
        "            ground_truth_transformed = transform(ground_truth_transcript)\n",
        "            predicted_transcript = \" \".join([segment.text for segment in segments_list])\n",
        "            predicted_transcript_transformed = transform(predicted_transcript)\n",
        "\n",
        "            wer = jiwer.wer(ground_truth_transformed, predicted_transcript_transformed)\n",
        "\n",
        "            end_time = time.time()\n",
        "            inference_time = end_time - start_time\n",
        "\n",
        "            if device == \"cuda\":\n",
        "                infoo = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "\n",
        "            results.append({\n",
        "                \"audio_file\": audio_file,\n",
        "                \"model_size\": config[\"size\"],\n",
        "                \"compute_type\": config[\"compute\"],\n",
        "                \"inference_time\": inference_time,\n",
        "                \"language\": info.language,\n",
        "                \"language_probability\": info.language_probability,\n",
        "                \"num_segments\": len(segments_list),\n",
        "                \"wer\": wer,\n",
        "                \"predicted_transcript\": predicted_transcript,\n",
        "                \"Used memory (GB)\": infoo.used / (1024 ** 3)\n",
        "            })\n",
        "\n",
        "            print(\n",
        "                f\"Audio: {audio_file}, Model: {config['size']}, Compute: {config['compute']}, Time: {inference_time:.2f}s, WER: {wer:.2f}, Text:, {predicted_transcript}, Used memory (GB): {infoo.used / (1024 ** 3)}\"\n",
        "            )\n",
        "\n",
        "            if wer == 0:\n",
        "                print(f\"Skipping remaining models for this audio\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(\n",
        "                f\"Error with Audio: {audio_file}, Model: {config['size']}, Compute: {config['compute']}: {e}\"\n",
        "            )\n",
        "            results.append({\n",
        "                \"audio_file\": audio_file,\n",
        "                \"model_size\": config[\"size\"],\n",
        "                \"compute_type\": config[\"compute\"],\n",
        "                \"inference_time\": \"Error\",\n",
        "                \"error\": str(e),\n",
        "                \"wer\": \"Error\",\n",
        "                \"predicted_transcript\": \"Error\",\n",
        "            })\n",
        "\n",
        "        finally:\n",
        "            del model\n",
        "            if config[\"size\"] == \"turbo\":\n",
        "                del batched_model\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "# Print or save results (e.g., to a CSV file)\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"whisper_benchmark_results_turbo.csv\", index=False)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "jKnpH-sU9j__",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a98f887-1d19-4285-83eb-4990f1ad91cb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA available: True\n",
            "Audio: edwardchrome.m4a, Model: turbo, Compute: float16, Time: 2.24s, WER: 0.00, Text:,  Hey Sora, open Chrome., Used memory (GB): 2.1092529296875\n",
            "Skipping remaining models for this audio\n",
            "Audio: edwardkahoot.m4a, Model: turbo, Compute: float16, Time: 2.50s, WER: 0.25, Text:,  Hey Sora, open kahut., Used memory (GB): 2.1092529296875\n",
            "         audio_file model_size compute_type  inference_time language  \\\n",
            "0  edwardchrome.m4a      turbo      float16        2.240465       en   \n",
            "1  edwardkahoot.m4a      turbo      float16        2.495633       en   \n",
            "\n",
            "   language_probability  num_segments   wer     predicted_transcript  \\\n",
            "0                     1             1  0.00   Hey Sora, open Chrome.   \n",
            "1                     1             1  0.25    Hey Sora, open kahut.   \n",
            "\n",
            "   Used memory (GB)  \n",
            "0          2.109253  \n",
            "1          2.109253  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Kesimpulan: turbo tidak digunakan karena memakan memori >2GB, yaitu melebihi kapasitas jetson nano 2GB"
      ],
      "metadata": {
        "id": "SUwERRFe0-zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<h1 >Trained benchmark"
      ],
      "metadata": {
        "id": "oy89Qiv6slT1"
      }
    },
    {
      "source": [
        "!pip install transformers torch jiwer datasets"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "SFoq0Mz-XrP2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be06bc75-1148-4293-f4bc-f6f0c780de63"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.49.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.4.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.29.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2024.12.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.1.8)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.12.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.14)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "source": [
        "\n",
        "import time\n",
        "import torch\n",
        "from transformers import WhisperProcessor, WhisperForConditionalGeneration\n",
        "from datasets import load_dataset\n",
        "import jiwer\n",
        "from jiwer import transforms\n",
        "import librosa\n",
        "import numpy as np\n",
        "\n",
        "# Load the processor and model\n",
        "processor = WhisperProcessor.from_pretrained(\"EdwardFang09/whisper-base-TA-2025_v2\")\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\"EdwardFang09/whisper-base-TA-2025_v2\").to(\"cuda\")\n",
        "\n",
        "# Load your audio data and ground truth transcripts (replace with your data)\n",
        "audio_ground_truth = {\n",
        "    \"edwardchrome.m4a\": \"sora, open chrome.\",\n",
        "    \"edwardkahoot.m4a\": \"sora, open kahoot.\",\n",
        "    # ... more audio files and transcripts\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for audio_file, ground_truth_transcript in audio_ground_truth.items():\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        device = \"cuda\" if config[\"compute\"]!= \"int8\" else \"cpu\"\n",
        "\n",
        "        if device == \"cuda\":\n",
        "          nvidia_smi.nvmlInit()\n",
        "          handle = nvidia_smi.nvmlDeviceGetHandleByIndex(0)\n",
        "          # No memory info printed here\n",
        "\n",
        "        # Load audio data using librosa\n",
        "        audio_data, sr = librosa.load(audio_file, sr=16000)  # Load audio at 16kHz\n",
        "\n",
        "        # Use the audio data (NumPy array) as input to the processor\n",
        "        input_features = processor(audio_data, sampling_rate=sr, return_tensors=\"pt\").input_features.to(\"cuda\")\n",
        "\n",
        "        # Generate predictions\n",
        "        predicted_ids = model.generate(input_features)\n",
        "        predicted_transcript = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "        # Calculate WER\n",
        "        transform = transforms.Compose([\n",
        "            transforms.ToLowerCase(),\n",
        "            transforms.RemovePunctuation(),\n",
        "            transforms.RemoveMultipleSpaces(),\n",
        "            transforms.Strip(),\n",
        "        ])\n",
        "\n",
        "        ground_truth_transformed = transform(ground_truth_transcript)\n",
        "        predicted_transcript_transformed = transform(predicted_transcript)\n",
        "        wer = jiwer.wer(ground_truth_transformed, predicted_transcript_transformed)\n",
        "\n",
        "        end_time = time.time()\n",
        "        inference_time = end_time - start_time\n",
        "\n",
        "        if device == \"cuda\":\n",
        "                infoo = nvidia_smi.nvmlDeviceGetMemoryInfo(handle)\n",
        "\n",
        "        results.append({\n",
        "            \"audio_file\": audio_file,\n",
        "            \"inference_time\": inference_time,\n",
        "            \"wer\": wer,\n",
        "            \"predicted_transcript\": predicted_transcript,\n",
        "            \"Used memory (GB)\": infoo.used / (1024 ** 3)\n",
        "        })\n",
        "\n",
        "        print(f\"Audio: {audio_file}, Time: {inference_time:.2f}s, WER: {wer:.2f}, Text: {predicted_transcript}, Used memory (GB): {infoo.used / (1024 ** 3)}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error with Audio: {audio_file}: {e}\")\n",
        "        results.append({\n",
        "            \"audio_file\": audio_file,\n",
        "            \"inference_time\": \"Error\",\n",
        "            \"error\": str(e),\n",
        "            \"wer\": \"Error\",\n",
        "            \"predicted_transcript\": \"Error\",\n",
        "            \"Used memory (GB)\": infoo.used / (1024 ** 3)\n",
        "        })\n",
        "\n",
        "# Save or print the results\n",
        "import pandas as pd\n",
        "df = pd.DataFrame(results)\n",
        "df.to_csv(\"huggingface_whisper_benchmark_trained.csv\", index=False)\n",
        "print(df)"
      ],
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rtEMen_s-6S1",
        "outputId": "5e5ad3d2-9de8-45e2-f0ec-7abc6f8789a2"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-a24218b242f1>:35: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(audio_file, sr=16000)  # Load audio at 16kHz\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio: edwardchrome.m4a, Time: 2.36s, WER: 0.25, Text: sora open chrome, Used memory (GB): 0.7440185546875\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-a24218b242f1>:35: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  audio_data, sr = librosa.load(audio_file, sr=16000)  # Load audio at 16kHz\n",
            "/usr/local/lib/python3.11/dist-packages/librosa/core/audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
            "\tDeprecated as of librosa version 0.10.0.\n",
            "\tIt will be removed in librosa version 1.0.\n",
            "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio: edwardkahoot.m4a, Time: 0.30s, WER: 0.25, Text: sora open kahoot, Used memory (GB): 0.7440185546875\n",
            "         audio_file  inference_time   wer predicted_transcript  \\\n",
            "0  edwardchrome.m4a        2.355231  0.25     sora open chrome   \n",
            "1  edwardkahoot.m4a        0.302079  0.25     sora open kahoot   \n",
            "\n",
            "   Used memory (GB)  \n",
            "0          0.744019  \n",
            "1          0.744019  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "buat csvnya dengan screen shot"
      ],
      "metadata": {
        "id": "-GKFPC1r0705"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dengan dataset yang sama, perhatikan perbedaan dengan model biasa."
      ],
      "metadata": {
        "id": "k0ts4utk1KlJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Solusi:\n",
        "1. Model di finetune supaya akurasi meningkat. <2 detik sudah relatif cepat.\n",
        "  - contoh: kahut, kehut, kuhut, dll.\n",
        "2. Cari banyak data untuk train dan testing. Pakai mikrofon ampas.\n"
      ],
      "metadata": {
        "id": "GwlN-YokAoPY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "need a code that fetch a whole data inside a folder"
      ],
      "metadata": {
        "id": "0q9a4S_zF0Yv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "optimized"
      ],
      "metadata": {
        "id": "fxt4wtcAFYb6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_properties(0)"
      ],
      "metadata": {
        "id": "DnsuS5X82MKs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb38566b-215d-4c1c-958f-0c23e0de60f5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "_CudaDeviceProperties(name='Tesla T4', major=7, minor=5, total_memory=15095MB, multi_processor_count=40, uuid=27ca1096-5681-b38d-b6fd-8d8f4b4ad7d2, L2_cache_size=4MB)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.get_device_properties(0).total_memory"
      ],
      "metadata": {
        "id": "UDvypiDC2NvV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d18cc5ea-1b23-4308-dd8a-e8840e1b700f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "15828320256"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "penemuan: turbo mendeteksi aksen bahasa indonesia walau sudah dicoding english dan berbicara bahasa inggris."
      ],
      "metadata": {
        "id": "_OvZXxHy1gRx"
      }
    }
  ]
}